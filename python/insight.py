# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hjMzghMaVSNlQ16buHAaKxw6XM7_PKe3
"""

import pandas as pd
import numpy as np
import re
import string
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud
from textblob import TextBlob
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA

# nltk.download('punkt')
nltk.download('punkt_tab')
nltk.download('stopwords')

# Mock data generation
mock_data = [
    {"extracted_text": "This project is about AI-based grading for assignments.", "final_score": 84, "classification": "high", "status": "selected"},
    {"extracted_text": "The system uses NLP to evaluate student responses.", "final_score": 78, "classification": "medium", "status": "not selected"},
    {"extracted_text": "Garbage classification using AI to improve waste management.", "final_score": 89, "classification": "high", "status": "selected"},
    {"extracted_text": "A chatbot that helps students answer common questions.", "final_score": 66, "classification": "low", "status": "pending"},
    {"extracted_text": "Blockchain for secure student records and credential verification.", "final_score": 91, "classification": "high", "status": "selected"}
]

# Convert to DataFrame
df = pd.DataFrame(mock_data)

# Preprocess text function
def preprocess_text(text):
    if not isinstance(text, str):
        return ""
    text = text.lower()
    text = re.sub(r'\d+', '', text)  # Remove numbers
    text = text.translate(str.maketrans("", "", string.punctuation))  # Remove punctuation
    words = word_tokenize(text)
    words = [word for word in words if word not in stopwords.words('english')]
    return " ".join(words)

# Apply preprocessing
df["cleaned_text"] = df["extracted_text"].apply(preprocess_text)

### Extracting Common Topics ###
def extract_common_topics(texts, top_n=10):
    vectorizer = TfidfVectorizer(stop_words="english")
    tfidf_matrix = vectorizer.fit_transform(texts)
    feature_names = vectorizer.get_feature_names_out()
    return feature_names[:top_n]

common_topics = extract_common_topics(df["cleaned_text"])
# print("**Common Topics:**", common_topics.tolist())

### Clustering Thought Patterns ###
def cluster_thought_patterns(texts, n_clusters=3):
    vectorizer = TfidfVectorizer(stop_words="english")
    tfidf_matrix = vectorizer.fit_transform(texts)
    kmeans = KMeans(n_clusters=n_clusters, random_state=42).fit(tfidf_matrix)
    return kmeans.labels_

df["cluster"] = cluster_thought_patterns(df["cleaned_text"])
# print("**Thought Clusters:**", df["cluster"].tolist())

### Visualization: Word Cloud ###
wordcloud = WordCloud(width=800, height=400, background_color="white").generate(" ".join(df["cleaned_text"]))
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis("off")
plt.title("Most Common Words in Submissions")
plt.show()

### Visualization: Thought Clusters (PCA for 2D Plot) ###
pca = PCA(n_components=2)
tfidf_matrix = TfidfVectorizer(stop_words="english").fit_transform(df["cleaned_text"])
reduced_data = pca.fit_transform(tfidf_matrix.toarray())

plt.figure(figsize=(8, 6))
sns.scatterplot(x=reduced_data[:, 0], y=reduced_data[:, 1], hue=df["cluster"], palette="viridis", s=100)
plt.xlabel("PCA Component 1")
plt.ylabel("PCA Component 2")
plt.title("Thought Clusters (K-Means)")
plt.legend(title="Cluster")
plt.show()

### Final Score Distribution ###
sns.histplot(df["final_score"], bins=10, kde=True)
plt.xlabel("Final Score")
plt.title("Final Score Distribution")
plt.show()

### Classification Breakdown ###
sns.countplot(x=df["classification"])
plt.title("Submission Classification (Low, Medium, High)")
plt.show()

### Selection Status Analysis ###
sns.countplot(x=df["status"])
plt.title("Submission Status (Selected, Not Selected, Pending)")
plt.show()