# -*- coding: utf-8 -*-
"""VideoSummary.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13lnMD-tW98O0h6keebw9ye8q_ajrx4JX
"""

!pip install opencv-python-headless pytesseract numpy torch librosa moviepy python-dotenv google-generativeai transformers

# Import required libraries
import os
import cv2
import pytesseract
import numpy as np
import torch
import librosa
import moviepy.editor as mp
import threading
import time
import google.generativeai as genai
from PIL import Image
from dotenv import load_dotenv
from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline
from google.colab import files

from google.colab import files

uploaded = files.upload()

# Load environment variables
load_dotenv("credentials.env")

google_api_key = os.getenv("GOOGLE_API_KEY")
if not google_api_key:
    raise ValueError("Error: GOOGLE_API_KEY is missing. Check your credentials.env file.")

genai.configure(api_key=google_api_key)

# Install Tesseract OCR
!apt-get install -y tesseract-ocr
pytesseract.pytesseract.tesseract_cmd = "/usr/bin/tesseract"

# Configure Whisper ASR Model
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
device_idx = 0 if torch.cuda.is_available() else -1

model_id = "openai/whisper-tiny.en"
asr_model = AutoModelForSpeechSeq2Seq.from_pretrained(model_id).to(device)
processor = AutoProcessor.from_pretrained(model_id)

asr_pipe = pipeline(
    "automatic-speech-recognition",
    model=asr_model,
    tokenizer=processor.tokenizer,
    feature_extractor=processor.feature_extractor,
    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
    device=device_idx,
)

class OCRProcessor:
    def __init__(self):
        pass

    def preprocess_image(self, image):
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        enhanced = cv2.convertScaleAbs(gray, alpha=1.5, beta=20)
        thresholded = cv2.adaptiveThreshold(enhanced, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2)
        return thresholded

    def extract_text(self, image):
        processed_image = self.preprocess_image(image)
        pil_image = Image.fromarray(processed_image)
        return pytesseract.image_to_string(pil_image, config="--psm 4 --oem 3").strip()

class VideoProcessor:
    def __init__(self, video_path, ocr_processor, threshold=30, frame_skip=10):
        self.video_path = video_path
        self.threshold = threshold
        self.frame_skip = frame_skip
        self.ocr = ocr_processor
        self.extracted_texts = set()

    def extract_text_from_video(self):
        cap = cv2.VideoCapture(self.video_path)
        if not cap.isOpened():
            print("Error: Cannot open video file.")
            return

        frame_count, prev_frame = 0, None
        while True:
            success, curr_frame = cap.read()
            if not success:
                break
            if frame_count % self.frame_skip == 0:
                if prev_frame is not None:
                    diff = cv2.absdiff(cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY),
                                       cv2.cvtColor(curr_frame, cv2.COLOR_BGR2GRAY))
                    if np.mean(diff) > self.threshold:
                        extracted_text = self.ocr.extract_text(curr_frame)
                        if extracted_text:
                            self.extracted_texts.add(extracted_text)
                prev_frame = curr_frame.copy()
            frame_count += 1
        cap.release()

    def get_extracted_text(self):
        return "\n".join(self.extracted_texts)

def extract_audio_from_video(video_path, output_audio_path="temp_audio.wav"):
    try:
        video = mp.VideoFileClip(video_path)
        video.audio.write_audiofile(output_audio_path, codec="pcm_s16le")
        return output_audio_path
    except Exception as e:
        print(f"Error extracting audio: {e}")
        return None

def transcribe_audio(audio_path, chunk_duration=30):
    audio, sr = librosa.load(audio_path, sr=16000)
    samples_per_chunk = chunk_duration * sr
    num_chunks = int(np.ceil(len(audio) / samples_per_chunk))
    chunks = [audio[i * samples_per_chunk: (i + 1) * samples_per_chunk] for i in range(num_chunks)]
    results = asr_pipe(chunks)
    return " ".join(result["text"] for result in results if "text" in result)

def summarize_text(text):
    if not text:
        return "No text to summarize."
    model = genai.GenerativeModel("gemini-2.0-flash")
    response = model.generate_content([{"text": f"Summarize the following text:\n\n{text}"}])
    return response.text if response.text else "Summarization failed."

def run_parallel(video_path):
    start_time = time.time()

    ocr_processor = OCRProcessor()
    video_processor = VideoProcessor(video_path, ocr_processor, threshold=35, frame_skip=15)

    transcript = ""

    def run_ocr():
        print("üìπ Running OCR on video frames...")
        video_processor.extract_text_from_video()

    def run_asr():
        nonlocal transcript
        print("üé§ Extracting and transcribing audio...")
        audio_file = extract_audio_from_video(video_path)
        if audio_file:
            transcript = transcribe_audio(audio_file)
            os.remove(audio_file)

    ocr_thread = threading.Thread(target=run_ocr)
    asr_thread = threading.Thread(target=run_asr)

    ocr_thread.start()
    asr_thread.start()

    ocr_thread.join()
    asr_thread.join()

    extracted_text = video_processor.get_extracted_text()
    text_summary = summarize_text(extracted_text)
    audio_summary = summarize_text(transcript)

    print("\nüìù Extracted Visual Text:")
    print(extracted_text)
    print("\nüìå Summary of Visual Text:")
    print(text_summary)
    print("\nüé§ Full Audio Transcript:")
    print(transcript)
    print("\nüîπ Summary of Audio:")
    print(audio_summary)
    print("\n‚è≥ Total Time Taken:", round(time.time() - start_time, 2), "seconds")

# Run the processing
video_file = "example.mp4"
run_parallel(video_file)